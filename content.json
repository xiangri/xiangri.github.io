[{"title":"Spark连接MongoDB--Python版本","date":"2017-05-11T14:46:33.000Z","path":"2017/05/11/Spark连接MongoDB-Python版本/","text":"最近发现Spark + MongoDB是个非常cool的组合，详见MongoDB + Spark: 完整的大数据解决方案 Spark连接MongoDB需要连接器,主要有官方和第三方两种，但是官方的连接器目前（写本文时）好像只支持Spark 1.6.x（我在2.1上折腾了几天都编译失败，这里需要注意下） 官方连接器 mongodb官方文档 第三方连接器 连接器详细的安装看上面连接的README就行了，第一次执行时会下载相应的工具包，但是国内”复杂网络环境”有可能会在build的时候失败(我就失败了好多次…)，我参考的文档里 作者提供了下载好的。 官方 第三方 下载解压后将cache、jar目录拷贝到~/.ivy2目录下即可。 选择器的选择 第三方连接器的功能比官方连接器的要好一点，支持在原有表的基础上做更新。 官方连接器的性能比第三方连接器的好一点，官方连接器和Spark有着相同的条件下推原则，会把过滤条件下推到MongoDB去执行。 选择器的使用12345#官方连接器运行：$spark-submit --packages org.mongodb.spark:mongo-spark-connector_2.10:1.1.0 text.py #第三方连接器运行：$spark-submit --packages com.stratio.datasource:spark-mongodb_2.10:0.11.2 text.py Spark-MongoDB的连接我使用的官方连接器，连接很简单：123456789conf = pyspark.SparkConf().setAppName(\"test\").setMaster(\"local\")sc = pyspark.SparkContext(conf=conf)sqls = SQLContext(sc) #database--数据库db#collection--数据库表/集合test_collection = sqls.read.format(\"com.mongodb.spark.sql\")\\ .options(uri=\"mongodb://127.0.0.1:27017\",database=\"runoob\",collection=\"col\").load() print(test_collection.first()) 更具体的参官方APISpark Connector Python Guide本文参考的文档写的非常棒spark处理mongodb数据（python版）","tags":[{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"},{"name":"MongoDB","slug":"MongoDB","permalink":"http://yoursite.com/tags/MongoDB/"},{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"}]},{"title":"����mongo-spark���ֵ�����","date":"2017-05-08T15:57:56.000Z","path":"2017/05/08/编译mongo-spark出现的问题/","text":"�� build the mongo-spark driverʱ�������´����� Waiting for lock on /home/hadoop/.sbt/boot/sbt.boot.lock to be available… Waiting for lock on /home/hadoop/.ivy2/.sbt.ivy.lock to be available… ����ͬʱ�����˶����ն˽���SBT�ı��룬�ŵ��½������Ļ��ƣ������ľ��ǽ��뵽��Ӧ��Ŀ¼�����⼸���ļ�����ɾ���������������롣 1234#��λ���������������ļ�λ�ã�Ȼ��ɾ��hadoop@ubuntu:~/.ivy2$ rm -rf .sbt.ivy.lock hadoop@ubuntu:~/.sbt/boot$ rm -rf sbt.boot.lock #���ɾͿ��Ա�����","tags":[{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"},{"name":"MongoDB","slug":"MongoDB","permalink":"http://yoursite.com/tags/MongoDB/"}]},{"title":"虚拟机下apt-get update更新出错","date":"2017-05-08T15:07:15.000Z","path":"2017/05/08/虚拟机apt-get-update更新出错/","text":"虚拟机下apt-get update更新出现some index files failed to download…软件源的问题。搬运工 http://jingyan.baidu.com/article/afd8f4de66efcf34e286e993.html 1、进入repogen.simplylinux.ch网站; 2、选择国家和自己装的Linux版本 3、将“ubuntu Branches”框内的选项全部打勾 4、网页拉到最下端，点击“Generate list” 5、用生成的源替换linux下 /etc/apt/sources.list中的内容 6、在linux终端下输入sudo apt-get update","tags":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"}]},{"title":"杀死Linux中的defunct进程","date":"2017-05-07T09:45:51.000Z","path":"2017/05/07/杀死Linux中的defunct进程/","text":"使用pyspark的时候非正常退出操作结果出现多个sparksubmit进程并且无法结束，pyspark也无法启动，纠结了半天才知道是defunct进程（僵尸进程）的原因,杀掉sparksubmit进程后pyspark就可以启动了。如图jps里出现多个SparkSubmit进程 直接kill &lt;进程号&gt; 发现并没有杀死，于是使用 kill -s 9 &lt;进程号&gt;再杀,查看java进程（ps -ef|grep java）。 发现还是没有杀死，但是刚kill的进程（1961）后面状态栏出现了个，于是查了下这个进程，原来这就是传说中的僵尸进程（关于这个不加介绍了），网上找了下杀死僵尸进程的两个办法： 重启计算机 使用kill -9 &lt;父进程号&gt; ，如果此法还没有只能重启了… 1234#查看java进程$ps -ef|grep java#会出现UID PID PPID ...#分别是用户ID 进程ID 父进程ID","tags":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"}]},{"title":"Ubuntu Spark 环境搭建","date":"2017-04-17T13:45:19.000Z","path":"2017/04/17/Ubuntu-Spark-环境搭建/","text":"Ubuntu-16.0-04-2 + spark2.1 + Anaconda3-4.1.1(python3.5.2) Anaconda安装12345678910#安装$ sudo bash Anacondaxxxxx.sh#之后依次会出现许可文件接收、询问是否将bin添加到环境变量，依次回车和yes即可#.bashrc的更新生效$ source ~/.bashrc#享用... $ python Spark安装基本和前一篇(Windows下部署spark)中Spark安装一致,下载、解压、添加环境变量。。。 Python中使用pyspark先将spark目录下的python库添加到了python的找寻目录，添加一个PYTHONPATH的环境变量 12#spark安装目录下python文件路径export PYTHONPATH=/home/hadoop/app/spark-2.1.0-bin-hadoop2.6/python 然后在spark安装目录下/spark-2.1.0-bin-hadoop2.6/python/lib里找到py4j-0.10.4-src.zip，将其解压到/spark-2.1.0-bin-hadoop2.6/python目录下即可。 #测试 import pyspark conf = pyspark.SparkConf().setAppName(\"test\").setMaster(\"local\") sc = pyspark.SparkContext(conf=conf)","tags":[{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"},{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"},{"name":"Ubuntu","slug":"Ubuntu","permalink":"http://yoursite.com/tags/Ubuntu/"},{"name":"Anaconda","slug":"Anaconda","permalink":"http://yoursite.com/tags/Anaconda/"}]},{"title":"Windows下部署spark","date":"2017-04-09T05:29:20.000Z","path":"2017/04/09/Windows下部署spark/","text":"Windows下部署spark部署纯属瞎折腾，开发还是linux。 部署环境windows10 + spark2.1 + python3.5 + Hadoop2.6 . 暂时不支持python3.6 ！！！(坑了好久。。。) 先装好JDK、Python和Hadoop环境。虽然Spark不依赖Hadoop但后面还是有坑… 安装Spark在Apache Spark™官网下载对应版本的spark. 下载完成解压(注意路径不要有空格！！！),然后添加spark环境变量 12345678#PATH后添加E:\\Spark\\binE:\\Spark\\sbin#下面这步至关重要，否则无法在python中使用spark#然后在环境变量里新建一个系统变量PYTHONPATHE:\\Spark\\python\\lib\\pyspark.zipE:\\Spark\\python\\lib\\py4j-0.10.4-src.zip 需要注意的是安装好后并不能用，，在window下还需要hadoop的winutil.exe，所以还需要下载hadoop版本对应的winutil.exe git上下载地址，将下载后的文件复制到hadoop的bin目录即可。 然后打开cmd 运行spark-shell看是否安装成功。 如果报以下错误1234Error occurred during initialization of VMCould not reserve enough space for object heapError: Could not create the Java Virtual Machine.Error: A fatal exception has occurred. Program will exit. 则需要指明JVM的内存分配限制，在系统环境变量中添加_JAVA_OPTIONS 前者为最小尺寸，初始分配;后者为最大允许分配尺寸，按需分配，最低得512MB才能启动。","tags":[{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"},{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"},{"name":"Hadoop","slug":"Hadoop","permalink":"http://yoursite.com/tags/Hadoop/"}]},{"title":"虚拟机连不上网络","date":"2017-04-07T09:45:54.000Z","path":"2017/04/07/虚拟机连不上网络/","text":"主机可以ping通虚拟机，虚拟机不能ping通主机 虚拟机里得ubuntu突然连不上网络了，搞了一个多小时最后发现是再ubuntu里手动改ipv4的时候不小心把网关敲错了。。。在设置ipv4的时候，ip前三段要和虚拟网络编辑器里相同，选择NAT模式设置时网关要和虚拟网络编辑器NAT设置里的网关一模一样。","tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"http://yoursite.com/tags/Ubuntu/"},{"name":"虚拟机","slug":"虚拟机","permalink":"http://yoursite.com/tags/虚拟机/"}]},{"title":"start-dfs.sh启动出错","date":"2017-04-05T13:26:17.000Z","path":"2017/04/05/start-dfs-sh启动出错/","text":"hadoop中start-dfs.sh启动时出错，报错： Permission denied解决办法：设置ssh免密码登陆 推测是ssh公钥认证没有启用，，但是安装ssh时在ssh的配置文件里面默认是启用公钥认证的，有点困惑。重新设置下ssh免密码登陆就可以了1234$ exit # 退出之前的 ssh localhost $ cd ~/.ssh/ # 若没有该目录，请先执行一次ssh localhost $ ssh-keygen -t rsa # 会有提示，都按回车就可以 $ cat id_rsa.pub &gt;&gt; authorized_keys # 加入授权","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://yoursite.com/tags/Hadoop/"},{"name":"ssh","slug":"ssh","permalink":"http://yoursite.com/tags/ssh/"}]},{"title":"虚拟机Ubuntu下Hadoop伪分布式安装","date":"2017-04-04T10:07:03.000Z","path":"2017/04/04/虚拟机Ubuntu下Hadoop伪分布式安装/","text":"VMware虚拟机Ubuntu下安装Hadoop及其配置. 使用secureCRT连接终端。 1、安装JDK.1.1 上传jdk安装包到服务器alt+p 出现sftp窗口，然后输入1$ put D:\\MyProgram\\JDK\\jdk-8u121-linux-x64.tar.gz 1.2 解压jdk1234#创建文件夹$ mkdir app#解压$ tar -zxvf jdk-8u121-linux-x64.tar.gz -C app 1.3 将java添加到环境变量中切换到su然后修改。1234567$ sudo vi /etc/profile#在文件最后添加export JAVA_HOME=/home/hadoop/app/jdk1.8.0_121export PATH=$PATH:$JAVA_HOME/bin #刷新配置生效$ source /etc/profile 2、安装Hadoop.2.1 上传hadoop安装包alt+p 出现sftp窗口，然后输入1$ put D:\\MyProgram\\hadoop-2.6.5.tar.gz 2.2 解压hadoop1234#创建文件夹$ mkdir app#解压$ tar -zxvf hadoop-2.6.5.tar.gz -C app 2.3 配置hadoop总共修改/hadoop-2.6.5/etc/hadoop/目录下五个文件 2.3.1 hadoop-env.sh123$ sudo vi hadoop-env.sh#将JAVA_HOME写死export JAVA_HOME=/usr/java/jdk1.7.0_65 2.3.2 core-site.xml1234567891011$ sudo vi core-site.xml#在&lt;configuration&gt;&lt;/configuration&gt;中插入&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://test:9000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/app/hadoop-2.6.5/tmp&lt;/value&gt;&lt;/property&gt; 2.3.3 hdfs-site.xml1234567$ sudo vi hdfs-site.xml#在&lt;configuration&gt;&lt;/configuration&gt;中插入&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; #节点个数&lt;/property&gt; 2.3.4 mapred-site.xml123456789#原文件为mapred-site.xml.template，需要先转换成xml文件$ mv mapred-site.xml.template mapred-site.xml$ sudo vi mapred-site.xml#在&lt;configuration&gt;&lt;/configuration&gt;中插入&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt; 2.3.5 yarn-site.xml1234567891011$ sudo vi yarn-site.xml#在&lt;configuration&gt;&lt;/configuration&gt;中插入&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;test&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt; 2.3.6 将hadoop添加到环境变量123456789$ sudo vi /etc/profile#在文件最后添加export JAVA_HOME=/home/hadoop/app/jdk1.8.0_121export HADOOP_HOME=/home/hadoop/app/hadoop-2.6.5export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin #刷新配置生效$ source /etc/profile 2.4 格式化namenode1$ hdfs namenode -format (或者hadoop namenode -format) 2.5 启动hadoop12$ start-dfs.sh$ start-yarn.sh 2.6 验证是否启动成功12345678910111213$ jps#下面六个全部有则成功27408 NameNode28218 Jps27643 SecondaryNameNode28066 NodeManager27803 ResourceManager27512 DataNode#web管理端http://ubuntu:50070 （HDFS管理界面）http://ubuntu:8088 （MR管理界面）","tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"http://yoursite.com/tags/Ubuntu/"},{"name":"Hadoop","slug":"Hadoop","permalink":"http://yoursite.com/tags/Hadoop/"},{"name":"虚拟机","slug":"虚拟机","permalink":"http://yoursite.com/tags/虚拟机/"}]},{"title":"Linux使用su切换用户提示Authentication failure","date":"2017-04-04T09:29:53.000Z","path":"2017/04/04/Linux使用su切换用户提示Authentication-failure/","text":"Linux使用su切换用户提示Authentication failure 这个问题产生的原因是由于ubuntu系统默认是没有激活root用户的，需要我门手工进行操作，在命令行界面下，或者在终端中输入如下命令：1234$ sudo passwdPassword：你当前的密码Enter new UNIX password：root密码Retype new UNIX password：重复root密码 然后会提示成功的信息. 【注】 使用su切换用户需要输入所切换到的用户的密码，而使用sudo则是当前用户的密码。","tags":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"}]},{"title":"secureCRT连接虚拟机上的Ubuntu失败","date":"2017-04-04T09:05:27.000Z","path":"2017/04/04/secureCRT连接虚拟机上的Ubuntu失败/","text":"两边IP都能ping通，但是都用secureCRT连接ubuntu是出现远程系统拒绝访问。 原因是ubuntu上没有ssh,所以先安装ssh。1$ sudo apt-get install openssh-server openssh-client 安装好后ssh服务是自动开启的，就可以正常连接了。如果还是连接不上可能是ssh服务没开，重新启动ssh-server $ /etc/init.d/ssh restart $ netstat -tlp tcp6 0 0 *:ssh *:* LISTEN - #看到上面这一行输出说明ssh-server已经在运行了。","tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"http://yoursite.com/tags/Ubuntu/"},{"name":"secureCRT","slug":"secureCRT","permalink":"http://yoursite.com/tags/secureCRT/"},{"name":"虚拟机","slug":"虚拟机","permalink":"http://yoursite.com/tags/虚拟机/"}]},{"title":"Navicat连接Oracle报错","date":"2017-03-28T12:49:16.000Z","path":"2017/03/28/Navicat连接Oracle报错/","text":"Navicat连接oracle报错ORA-12514: TNS:listener does not currently know of service requested in connect 1、Navicat和oracle版本需要比配，可以通过instantclient来解决。2、修改product\\11.2.0\\dbhome_1\\NETWORK\\ADMIN\\listener.ora文件1234567891011121314SID_LIST_LISTENER = (SID_LIST = (SID_DESC = (SID_NAME = CLRExtProc) (ORACLE_HOME = E:\\app\\Administrator\\product\\11.2.0\\dbhome_1) (PROGRAM = extproc) (ENVS = \"EXTPROC_DLLS=ONLY:E:\\app\\Administrator\\product\\11.2.0\\dbhome_1\\bin\\oraclr11.dll\") )/*添加下面这段 (SID_DESC = (GLOBAL_DBNAME = ORCL) (ORACLE_HOME = E:\\app\\Administrator\\product\\11.2.0\\dbhome_1) (SID_NAME = ORCL) )*/ )","tags":[{"name":"Navicat","slug":"Navicat","permalink":"http://yoursite.com/tags/Navicat/"},{"name":"Oracle","slug":"Oracle","permalink":"http://yoursite.com/tags/Oracle/"}]},{"title":"dot绘图","date":"2017-01-16T08:03:46.000Z","path":"2017/01/16/dot绘图/","text":"在用python实现决策树算法的时候发现可以利用dot和graphviz来可视化生成的决策树如下图，于是了解了下dot。 DOT语言是一种文本图形描述语言。它提供了一种简单的描述图形的方法，并且可以为人类和计算机程序所理解。 例如上图只需如下几行简单的文本：12345678910digraph G &#123; a -&gt; b -&gt; c; a -&gt; d; a -&gt; e; c -&gt; f; c -&gt; g; d -&gt; f; a -&gt; g; c -&gt; h;&#125; 然后利用graphviz软件将dot文件转化为png、pdf、gif等目标文件就可以可视化图片了。1dot -Tpng file.dot -o file.png 简单用法 有向图 1234graph graphname &#123; a -- b -- c; b -- d;&#125; 无向图 1234graph graphname &#123; a -&gt; b -&gt; c; b -&gt; d;&#125; 属性DOT语言中，可以对节点和边添加不同的属性。这些属性可以控制节点和边的显示样式，例如颜色，形状和线形。可以在语句和句尾的分号间放置一对方括号，并在其中中放置一个或多个属性-值对。多个属性可以被逗号和空格（,）。节点的属性被放置在只包含节点名称的表达式后。 123456789graph graphname &#123; // label属性可以改变节点的显示名称 a [label=\"Foo\"]; // 节点形状被改变了 b [shape=box]; // a-b边和b-c边有相同的属性 a -- b -- c [color=blue]; b -- d [style=dotted];&#125; 注释DOT语言支持C语言与C++风格的单行与多行注释。另外，也支持Shell脚本风格的以#开头的注释。 子图使用subgraph定义子流程图； 常用属性对于各种结构的通用的属性如下： 属性名称 默认值 含义 color black 颜色 colorscheme X11 颜色描述 fontcolor black 文字颜色 fontname Times-Roman 字体 fontsize 14 文字大小 label 显示的标签，对于节点默认为节点名称 penwidth 1.0 线条宽度 style 1.0 样式 weight 1.0 重要性 常用边属性如下： 属性名称 默认值 含义 arrowhead normal 箭头头部形状 arrowsize 1.0 箭头大小 arrowtail normal 箭头尾部形状 constraint true 是否根据边来影响节点的排序 decorate 用一条线来连接edge和label dir forward 设置方向：forward,back,both,none headclip true 是否到边界为止 tailclip true 与headclip类似 常用节点属性如下： 属性名称 默认值 含义 shape ellipse 形状 sides 4 当shape=polygon时的边数 fillcolor lightgrey/black 填充颜色 fixedsize false 标签是否影响节点的大小 常用图属性如下： 属性名称 默认值 含义 bgcolor 背景颜色 concentrate false 让多条边有公共部分 nodesep .25 节点之间的间隔（英寸） peripheries 1 边界数 rank same,min,source, max,sink，设置多个节点顺序 rankdir TB 排序方向 ranksep .75 间隔 size 图的大小（英寸） 高级用法 使用record的label属性生成表格用一对双引号+一对花括号包含起来的就是表格内容，不同的格子之间用符号 | 隔开，尖括号里的内容表示一个锚点，如下图。 对应代码： 12345678digraph structs &#123; node [shape=record]; struct1 [shape=record,label=\"&lt;f0&gt; left|&lt;f1&gt; mid\\ dle|&lt;f2&gt; right\"]; struct2 [shape=record,label=\"&lt;f0&gt; one|&lt;f1&gt; two\"]; struct3 [shape=record,label=\"hello\\nworld |&#123; b |&#123;c|&lt;here&gt; d|e&#125;| f&#125;| g | h\"]; struct1 -&gt; struct2; struct1 -&gt; struct3;&#125; label支持HTML格式 表格锚点的应用cell的锚点可以让使用者在cell之间画线。引用cell的锚点的语法为：table: anchor_name 示例代码：1234567digraph example2 &#123; node [shape = record]; table1 [label = \"&#123;&lt;head&gt;cell1 | cell2 | cell3&#125;\"]; table2 [label = \"&#123;&lt;head&gt;cell1 | cell2&#125;\"]; table1: head -&gt; table2: head;&#125; 总结graphviz的节点出现在画布上的位置事实上是不确定的，依赖于所使用的布局算法，而不是在脚本中出现的位置，这可能使刚开始接触graphviz的开发人员有点不适应。graphviz的强项在于自动布局，当图中的顶点和边的数目变得很多的时候，才能很好的体会这一特性的好处。","tags":[{"name":"dot","slug":"dot","permalink":"http://yoursite.com/tags/dot/"},{"name":"绘图","slug":"绘图","permalink":"http://yoursite.com/tags/绘图/"}]},{"title":"爬虫简单入门","date":"2016-11-29T01:32:06.000Z","path":"2016/11/29/爬虫简单入门/","text":"最近需要一些热点舆情的数据集，于是学习用爬虫简单的爬点数据…研究了下《Python3WebSpider》 使用urllib.request发送请求urllib.request 模块提供了最基本的构造 HTTP 请求的方法，利用它可以模拟浏览器的一个请求发起过程，同时它还带有处理 authenticaton （授权验证）， redirections （重定向)， cookies (浏览器Cookies）以及其它内容。urllib.request.urlopen()返回一个HTTPResposne 类型的对象，它主要包含的方法有 read() 、 readinto() 、 getheader(name) 、 getheaders() 、 fileno() 等函数和 msg 、 version 、 status 、 reason 、 debuglevel 、 closed 等属性。e.g.1234567import urllib.requestresponse = urllib.request.urlopen('https://www.baidu.com')print(type(response))print(response.read().decode('utf-8')) #read()返回网页内容print(response.status) #响应的状态码print(response.getheaders()) #响应的头信息print(response.getheader('Server')) #通过传递一个参数获取了 Server 的类型 urllib.request.urlopen()详解简单的调用urlopen(url),可以完成最基本的简单网页的 GET 请求抓取。复杂点的urlopen()API： urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None) data参数是可选的，如果要添加data，它要是字节流编码格式的内容，即bytes类型，通过 bytes() 函数可以进行转化，另外如果你传递了这个data参数，它的请求方法就不再是GET方式请求，而是POST。e.g.12345import urllib.parseimport urllib.requestdata = bytes(urllib.parse.urlencode(&#123;'word': 'hello'&#125;), encoding='utf8')response = urllib.request.urlopen('http://httpbin.org/post',data = data)print(response.read()) timeout 参数可以设置超时时间，单位为秒，意思就是如果请求超出了设置的这个时间还没有得到响应，就会抛出异常，如果不指定，就会使用全局默认时间。它支持 HTTP 、 HTTPS 、 FTP 请求。 其他参数还有 context 参数，它必须是 ssl.SSLContext 类型，用来指定 SSL 设置。cafile 和 capath 两个参数是指定CA证书和它的路径，这个在请求 HTTPS 链接时会有用。cadefault 参数现在已经弃用了，默认为 False 。More info: 官方文档 urllib.request.Request的使用Request()API: class urllib.request.Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False, method=None) url是请求链接，这个是必传参数，其他的都是可选参数。data 参数如果要传必须传 bytes （字节流）类型的，如果是一个字典，可以先用 urllib.parse.urlencode() 编码。headers 参数是一个字典，你可以在构造 Request 时通过 headers 参数传递，也可以通过调用 Request 对象的 add_header() 方法来添加请求头。请求头最常用的用法就是通过修改 User-Agent 来伪装浏览器，默认的 User-Agent 是 Python-urllib ，你可以通过修改它来伪装浏览器，比如要伪装火狐浏览器，你可以把它设置为 Mozilla/5.0 (X11; U; Linux i686)Gecko/20071127 Firefox/2.0.0.11 origin_req_host 指的是请求方的 host 名称或者 IP 地址。unverifiable 指的是这个请求是否是无法验证的，默认是 False 。意思就是说用户没有足够权限来选择接收这个请求的结果。例如我们请求一个HTML文档中的图片，但是我们没有自动抓取图像的权限，这时 unverifiable 的值就是 True 。method 是一个字符串，它用来指示请求使用的方法，比如 GET ， POST ， PUT 等等。e.g.1234567891011121314from urllib import request, parseurl = 'http://httpbin.org/post'headers = &#123; 'User-Agent': 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)', 'Host': 'httpbin.org'&#125;dict = &#123; 'name': 'Germey'&#125;data = bytes(parse.urlencode(dict), encoding='utf8')req = request.Request(url=url, data=data, headers=headers, method='POST')response = request.urlopen(req)print(response.read().decode('utf-8')) urllib.request高级特性Handler简而言之你可以把它理解为各种处理器，有专门处理登录验证的，有处理 Cookies 的，有处理代理设置的，利用它们我们几乎可以做到任何 HTTP 请求中所有的事情。首先介绍下 urllib.request.BaseHandler ，它是所有其他 Handler 的父类，它提供了最基本的 Handler 的方法，例如 default_open() 、 protocol_request() 等。接下来就有各种 Handler 类继承这个 BaseHandler ，列举如下： HTTPDefaultErrorHandler 用于处理HTTP响应错误，错误都会抛出 HTTPError 类型的异常。 HTTPRedirectHandler 用于处理重定向。HTTPCookieProcessor 用于处理 Cookie 。 ProxyHandler 用于设置代理，默认代理为空。HTTPPasswordMgr 用于管理密码，它维护了用户名密码的表。 HTTPBasicAuthHandler 用于管理认证，如果一个链接打开时需要认证，那么可以用它来解决认证问题。*另外还有其他的 Handler ，可以参考官方文档。 认证12345678910import urllib.requestauth_handler = urllib.request.HTTPBasicAuthHandler()#添加用户名和密码，建立了一个处理认证的处理器。auth_handler.add_password(realm = 'PDQ Application', uri ='https://mahler:8092/site-updates.py', user='klem', passwd='kadidd!ehopper')opener = urllib.request.build_opener(auth_handler) #具备认证功能的Openerurllib.request.install_opener(opener)urllib.request.urlopen('http://www.example.com/login.html') 代理123456789import urllib.request#ProxyHandler参数为字典key是协议类型，比如 http 还是 https等，value是代理链接，可以添加多个代理proxy_handler = urllib.request.ProxyHandler(&#123; 'http': 'http://218.202.111.10:80', #示例 'https': 'https://180.250.163.34:8888'&#125;)opener = urllib.request.build_opener(proxy_handler)response = opener.open('https://www.baidu.com')print(response.read()) Cookie设置怎样将网站的Cookie获取下来1234567import http.cookiejar, urllib.requestcookie = http.cookiejar.CookieJar() #CookieJar对象handler = urllib.request.HTTPCookieProcessor(cookie)opener = urllib.request.build_opener(handler)response = opener.open('http://www.baidu.com')for item in cookie: print(item.name+\"=\"+item.value) 将网站Cookie设置到headers里，可以实现登陆。 requests的基本使用123456import requestsurl = 'http://www.baidu.com'r = requests.get(url) #GET 请求print(r.status_code) #状态码print(r.text) #响应体内容print(r.cookies) #Cookies GET请求HTTP中最常见的请求之一就是 GET 请求，我们首先来详细了解下利用requests来构建GET请求的方法以及相关属性方法操作。首先让我们来构建一个最简单的 GET 请求，请求httpbin.org/get它会判断如果你是GET请求的话，会返回响应的请求信息。123import requestsr = requests.get('http://httpbin.org/get')print(r.text) 运行结果如下：1234567891011&#123; \"args\": &#123;&#125;, \"headers\": &#123; \"Accept\": \"*/*\", \"Accept-Encoding\": \"gzip, deflate\", \"Host\": \"httpbin.org\", \"User-Agent\": \"python-requests/2.10.0\"&#125;, \"origin\": \"122.4.215.33\", \"url\": \"http://httpbin.org/get\"&#125; 可以发现我们成功发起了get请求，请求的链接和头信息都有相应的返回。如果现在我想添加两个参数，名字name是germey，年龄age是22。那么可以利用params参数e.g.1234567import requestsdata = &#123; 'name': 'germey', 'age': 22&#125;r = requests.get(\"http://httpbin.org/get\", params=data)print(r.text) 运行结果如下：1234567891011121314&#123; \"args\": &#123; \"age\": \"22\", \"name\": \"germey\"&#125;, \"headers\": &#123; \"Accept\": \"*/*\", \"Accept-Encoding\": \"gzip, deflate\", \"Host\": \"httpbin.org\", \"User-Agent\": \"python-requests/2.10.0\"&#125;, \"origin\": \"122.4.215.33\", \"url\": \"http://httpbin.org/get?age=22&amp;name=germey\"&#125; 通过返回信息我们可以判断，请求的链接自动被构造成了http://httpbin.org/get?age=22&amp;name=germey. 另外，网页的返回类型实际上是str类型，但是它很特殊，是Json的格式，所以如果我们想直接把返回结果解析，得到一个字典dict格式的话，可以直接调用json()方法,就可以将返回结果是Json格式的字符串转化为字典dict。但是注意如果返回结果不是Json格式，便会出现解析错误，抛出抛json.decoder.JSONDecodeError 的异常。 抓取二进制数据我们都知道，图片、音频、视频这些文件都是本质上由二进制码组成的，由于有特定的保存格式和对应的解析方式，我们才可以看到这些形形色色的多媒体。所以想要抓取他们，那就需要拿到他们的二进制码。e.g.GitHub的站点图标1234import requestsr = requests.get(\"https://github.com/favicon.ico\")print(r.text)print(r.content) 运行代码可以发现r.text的代码出现乱码，而r.content的结果前面带有一个b，代表这是bytes类型的数据。由于图片是二进制数据，所以前者在打印时转化为 str 类型，也就是图片直接转化为字符串，理所当然会出现乱码。两个属性有什么区别？前者返回的是字符串类型，如果返回结果是文本文件，那么用这种方式直接获取其内容即可。如果返回结果是图片、音频、视频等文件， requests 会为我们自动解码成 bytes 类型，即获取字节流数据。123456import requestsr = requests.get(\"https://github.com/favicon.ico\")# open() 函数，第一个参数是文件名称，第二个参数代表以二进制写的形式打开，可以向文件里写入二进制数据，然后保存.with open('favicon.ico', 'wb') as f:f.write(r.content)f.close() 运行结束之后，可以发现在文件夹中出现了名为favicon.ico的图标,同样的，音频、视频文件也可以用这种方法获取。 状态码requests 还提供了一个内置的状态码查询对象 requests.codes 。比如你可以通过 if r.status_code ==requests.codes.ok 来判断请求是否成功.返回码和相应的查询条件:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273# Informational.100: ('continue',),101: ('switching_protocols',),102: ('processing',),103: ('checkpoint',),122: ('uri_too_long', 'request_uri_too_long'),200: ('ok', 'okay', 'all_ok', 'all_okay', 'all_good', '\\\\o/', '✓'),201: ('created',),202: ('accepted',),203: ('non_authoritative_info', 'non_authoritative_information'),204: ('no_content',),205: ('reset_content', 'reset'),206: ('partial_content', 'partial'),207: ('multi_status', 'multiple_status', 'multi_stati', 'multiple_stati'),208: ('already_reported',),226: ('im_used',),300: ('multiple_choices',),301: ('moved_permanently', 'moved', '\\\\o-'),302: ('found',),303: ('see_other', 'other'),304: ('not_modified',),305: ('use_proxy',),306: ('switch_proxy',),307: ('temporary_redirect', 'temporary_moved', 'temporary'),308: ('permanent_redirect','resume_incomplete', 'resume',), # These 2 to be removed in 3.0# Client Error.400: ('bad_request', 'bad'),401: ('unauthorized',),402: ('payment_required', 'payment'),403: ('forbidden',),404: ('not_found', '-o-'),405: ('method_not_allowed', 'not_allowed'),406: ('not_acceptable',),407: ('proxy_authentication_required', 'proxy_auth', 'proxy_authentication'),408: ('request_timeout', 'timeout'),409: ('conflict',),410: ('gone',),411: ('length_required',),412: ('precondition_failed', 'precondition'),413: ('request_entity_too_large',),414: ('request_uri_too_large',),415: ('unsupported_media_type', 'unsupported_media', 'media_type'),416: ('requested_range_not_satisfiable', 'requested_range','range_not_satisfiable'),417: ('expectation_failed',),418: ('im_a_teapot', 'teapot', 'i_am_a_teapot'),421: ('misdirected_request',),422: ('unprocessable_entity', 'unprocessable'),423: ('locked',),424: ('failed_dependency', 'dependency'),425: ('unordered_collection', 'unordered'),426: ('upgrade_required', 'upgrade'),428: ('precondition_required', 'precondition'),429: ('too_many_requests', 'too_many'),431: ('header_fields_too_large', 'fields_too_large'),444: ('no_response', 'none'),449: ('retry_with', 'retry'),450: ('blocked_by_windows_parental_controls', 'parental_controls'),451: ('unavailable_for_legal_reasons', 'legal_reasons'),499: ('client_closed_request',),# Server Error.500: ('internal_server_error', 'server_error', '/o\\\\', '✗'),501: ('not_implemented',),502: ('bad_gateway',),503: ('service_unavailable', 'unavailable'),504: ('gateway_timeout',),505: ('http_version_not_supported', 'http_version'),506: ('variant_also_negotiates',),507: ('insufficient_storage',),509: ('bandwidth_limit_exceeded', 'bandwidth'),510: ('not_extended',),511: ('network_authentication_required', 'network_auth', 'network_authentication'), 未完待续…","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/爬虫/"}]},{"title":"Python3 Error合集","date":"2016-11-28T10:59:38.000Z","path":"2016/11/28/Python3-Error合集/","text":"一些遇到过的python error解决方法。 UnicodeEncodeError用urllib和BeautifulSoup抓取网页的时候，打印出来结果报错： UnicodeEncodeError: ‘gbk’ codec can’t encode character ‘\\xbb’ in position 26322: illegal multibyte sequence 检查了下要抓取的网页源码发现网页也是用的utf-8，于是查了下发现是print()函数自身有限制(python默认编码的局限)，不能完全打印所有的unicode字符。解决方法一：打印的时候用.decode(‘utf-8’)。解决方法二：改下python的默认编码。1sys.stdout = io.TextIOWrapper(sys.stdout.buffer,encoding='utf8') #改变标准输出的默认编码 urllib.error.HTTPError: HTTP Error 403: Forbidden用 urllib.request.urlopen 方式打开一个URL,服务器端只会收到一个单纯的对于该页面访问的请求,但是服务器并不知道发送这个请求使用的浏览器,操作系统,硬件平台等信息,而缺失这些信息的请求往往都是非正常的访问,例如爬虫.有些网站为了防止这种非正常的访问,会验证请求信息中的UserAgent(它的信息包括硬件平台、系统软件、应用软件和用户个人偏好),如果UserAgent存在异常或者是不存在,那么这次请求将会被拒绝(如上错误信息所示)所以可以尝试在请求中加入UserAgent的信息解决方法：改下python的默认编码。12345#如果不加上下面的这行出现会出现urllib2.HTTPError: HTTP Error 403: Forbidden错误 #主要是由于该网站禁止爬虫导致的，可以在请求加上头信息，伪装成浏览器访问User-Agent,具体的信息可以通过火狐的FireBug插件查询 headers = &#123;'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:23.0) Gecko/20100101 Firefox/23.0'&#125; req = urllib.request.Request(url=chaper_url, headers=headers) urllib.request.urlopen(req).read()","tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"},{"name":"Error","slug":"Error","permalink":"http://yoursite.com/tags/Error/"}]},{"title":"ACM酱油记.","date":"2016-11-22T14:22:22.000Z","path":"2016/11/22/ACM酱油记./","text":"退役了。。。 在集训队划了这么久水，名义上搞了两年，但其实真正AC过的题很少，大多都是看完题想出思路了然后一看正解也是这样就懒得去写代码了，这也导致了到了真正比赛的时候总是理（zui）论（pao）AC，思路正解的题总是各种原因wa，最终CCPC杭州和ICPC香港都坑了队友打了两块铁。我这么懒其实不适合ACM这个比赛，没拿到区域赛奖我觉得反而是件好事，让我明白了很多道理，以后的生活更有目标和动力了吧。以前一直认为“真正努力过，才明白天赋有多重要”，道理没错，然而事实上绝大多数人努力的程度之低压根轮不到谈什么天赋。仔细想想自己这两年，别说什么天赋，连努力都谈不上，真是浪费了两年在各种逃课打游戏、无效社交等等上。先定个小目标，做自己真正喜欢做的事，然后努力到拼天赋的程度，苟，岂。","tags":[{"name":"ACM-ICPC","slug":"ACM-ICPC","permalink":"http://yoursite.com/tags/ACM-ICPC/"},{"name":"杂记","slug":"杂记","permalink":"http://yoursite.com/tags/杂记/"}]},{"title":"hexo shell & Markdown","date":"2016-11-22T09:22:05.000Z","path":"2016/11/22/hexo shell & Markdown/","text":"一些常用的Hexo命令 &amp; Markdown_基本语法. Create a new post1$ hexo new \"My New Post\" More info: Writing Delete a post12345$ 删除/source/_posts/xxx.md$ 删掉/hexo/db.json文件$ hexo clean$ hexo generate$ hexo deploy Server1234567$ hexo server #Hexo 会监视文件变动并自动更新，您无须重启服务器。$ hexo server -s #静态模式$ hexo server -p 5000 #更改端口$ hexo server -i 192.168.1.1 #自定义 IP$ hexo clean #清除缓存 网页正常情况下可以忽略此条命令$ hexo generate #生成静态网页$ hexo deploy #开始部署 More info: Server Deployment ERROR1$ ERROR Deployer not found: git 1$ npm install hexo-deployer-git --save Markdown Markdown_基本语法","tags":[{"name":"hexo-shell-Markdown","slug":"hexo-shell-Markdown","permalink":"http://yoursite.com/tags/hexo-shell-Markdown/"}]}]